# Extended-Essay

The focus of this essay is to investigate computational complexities and regrets of
algorithms addressing the Multi-Armed Bandit Problem (MAPB), a famous problem for
analyzing the exploration-exploitation tradeoff. Today, the exploration-exploitation tradeoff
dilemma is present in many settings, such as online advertising, healthcare, and finance (A
Survey on Practical Applications). This essay will specifically look into Thompson Sampling
(TS) and Upper Confidence Bound (UCB) algorithms. These algorithms would be investigated
in terms of time complexity––the time taken for an algorithm to run given a set of input values of
a certain size––and the total expected regret––the difference between rewards yielded from the
optimal action and from the chosen action. Hence, the question: “How does the performance of
Thompson Sampling compare to that of the Upper Confidence Bound Algorithm in terms of time
complexity and regret?”

Full essay can be found in EE.pdf

## How to run

The experiments were ran in newmain.ipynb and the data are saved into time.csv and regret.csv

Implementations of Thompson Sampling, UCB, and the bandits could be found in Model.py

![pdf](pngs/EE-01.png)
![pdf](pngs/EE-02.png)
![pdf](pngs/EE-03.png)
![pdf](pngs/EE-04.png)
![pdf](pngs/EE-05.png)
![pdf](pngs/EE-06.png)
![pdf](pngs/EE-07.png)
![pdf](pngs/EE-08.png)
![pdf](pngs/EE-09.png)
![pdf](pngs/EE-10.png)
![pdf](pngs/EE-11.png)
![pdf](pngs/EE-12.png)
![pdf](pngs/EE-13.png)
![pdf](pngs/EE-14.png)
![pdf](pngs/EE-15.png)
![pdf](pngs/EE-16.png)
![pdf](pngs/EE-17.png)
![pdf](pngs/EE-18.png)
![pdf](pngs/EE-19.png)
![pdf](pngs/EE-20.png)
![pdf](pngs/EE-21.png)
![pdf](pngs/EE-22.png)
![pdf](pngs/EE-23.png)
![pdf](pngs/EE-24.png)
![pdf](pngs/EE-25.png)
![pdf](pngs/EE-26.png)
![pdf](pngs/EE-27.png)
![pdf](pngs/EE-28.png)
![pdf](pngs/EE-29.png)
![pdf](pngs/EE-30.png)
